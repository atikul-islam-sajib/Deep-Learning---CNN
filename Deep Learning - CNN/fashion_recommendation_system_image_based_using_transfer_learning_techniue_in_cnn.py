# -*- coding: utf-8 -*-
"""Fashion Recommendation System Image Based using Transfer Learning Techniue in CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16UlKv7zjuiJopGmB6A_YrKDwETGuvkP_
"""

##### Import all necessity functions for Machine Learning #####
import sys
import math
import string
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as shc
import warnings
import zipfile
import cv2
import os
import random
from collections import Counter
from functools import reduce
from itertools import chain
from google.colab.patches import cv2_imshow
from keras.preprocessing import image
from sklearn.metrics._plot.confusion_matrix import confusion_matrix
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, SelectKBest, chi2, VarianceThreshold
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTEN, SMOTENC, SVMSMOTE, KMeansSMOTE, BorderlineSMOTE, ADASYN
from imblearn.ensemble import EasyEnsembleClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, NearestNeighbors
from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier, SGDRegressor, Perceptron
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier, ExtraTreeRegressor
from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, VotingClassifier, VotingRegressor
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor, StackingClassifier, StackingRegressor
from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, recall_score, precision_score, f1_score, silhouette_score
from xgboost import XGBClassifier, XGBRegressor

##### Download keras #####
!pip install keras

##### Import all necessity functions for Neural Network #####
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Dense, Conv2D, LSTM, GRU, RNN, Flatten, AvgPool2D, MaxPool2D, GlobalAveragePooling2D, BatchNormalization, Dropout, LeakyReLU, ELU, PReLU
from tensorflow.keras.activations import tanh, relu, sigmoid, softmax, swish
from tensorflow.keras.regularizers import L1, L2, L1L2
from tensorflow.keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam, Adamax, Nadam
from tensorflow.keras.initializers import HeNormal, HeUniform, GlorotNormal, GlorotUniform
from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy, hinge, MSE, MAE, Huber
import keras.utils as image

##### Remove all warnings #####
import warnings
warnings.filterwarnings("ignore")


##### Using, NearestNeighbours, we will find the closest point of that particular input #####
from sklearn.neighbors import NearestNeighbors

##### To Access the Google Drive #####
def google_drive(parameter = None):
  try:
    from google.colab import drive
    drive.mount('/content/drive',force_remount=True)
  except Exception as e:
    print(e.with_traceback)
  else:
    print('\nGoogle Drive access is done.\n'.title())

##### Execute this from the current directory #####
if __name__ == "__main__":
  google_drive()

##### To Unzip the folder #####
def unzip_file(parameter_ = None):
  try:
    link_folder_ = '/content/drive/MyDrive/CNN Dataset/archive (32).zip'
    zip_ref = zipfile.ZipFile(link_folder_, 'r')
    zip_ref.extractall()
    zip_ref.close()
  except Exception as e:
    print(e.with_traceback)
  else:
    print('Upzip is done succesfully'.title())

##### Execute it from the current directory #####
if __name__ == "__main__":
  unzip_file()

##### Create the categories by hand ####
_CATEGORIES = []
for value in range(10, 96):
    _CATEGORIES.append("0"+str(value))

##### Print this to see whether error occurs or not #####
print(_CATEGORIES, end = " ")

##### Directory of the image #####
_DIRECTORY = "/content/images_128_128"
##### For further use, create a list there all the image path would be stored #####
IMAGE_PATH = []

##### Join the Folder of the images #####
for categories in _CATEGORIES:
  ##### Make the folder path #####
  folder_path = os.path.join(_DIRECTORY, categories)
  ##### Make sure all the images along with folder path appeared #####
  for images in os.listdir(folder_path):
    ##### Extract the each images to the list #####
    image_path = os.path.join(folder_path, images)
    ##### appending the image path into the list for further use #####
    IMAGE_PATH.append(image_path)
##### Show the categories that are extracted successfully #####
  print('{} is done.\n'.format(categories))

##### Print the total number of Images of this dataset #####
print('The total images in this dataset is {}'.format(len(IMAGE_PATH)))

##### Use only 40K dataset for training #####
"""
The problem is that, we run with 105100 dataset but it crashes most of the time.
"""
def activate(activated = True, IMAGE_PATH = None):
  if activated == True:
    ##### Shuffle the dataset #####
    random.shuffle(IMAGE_PATH)
    ##### Take only 40k image daytaset to train #####
    IMAGE_PATH = IMAGE_PATH[0:40000]
    ##### return the dataset #####
    return IMAGE_PATH

  else:
    raise Exception('Activation is not possible'.capitalize())

##### call the function #####
if __name__ =="__main__":
  try:
    IMAGE_PATH = activate(True, IMAGE_PATH)
  except Exception as e:
    print(e)
  else:
    print('Done with successfully.'.capitalize())
    print('The number of dataset is {}'.format(len(IMAGE_PATH)))

"""ResNet50 - Transfer Learning is extracted """

##### Call the ResNet50 architecture #####
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input

##### Define a ResNet50 function #####
def restnet50(call = None):
  if call == True:
    ##### Create an object of ResNet50 #####
    ResNet_ = ResNet50(include_top = False, \
                      weights = 'imagenet', \
                       input_shape = (128, 128, 3))

    ##### Freeze the trainable parameter #####
    ResNet_.trainable = False
    ##### Show the model summary #####
    ResNet_.summary()
    ##### Return the object of ResNet50 #####
    return ResNet_

##### Execute it from the current directory #####
if __name__ == "__main__":
  ResNet_ = restnet50(True)

##### Create the sequential model #####
model = Sequential()
##### ResNet50 object should be added into the sequential model ####
model.add(ResNet_)
##### Use the Global Pooling Layer as if translation invariance problem prevented - Higher order features is extracted #####
model.add(GlobalAveragePooling2D())

##### Show the model summary #####
model.summary()

##### plot the model #####
from tensorflow.keras.utils import plot_model
plot_model( model,\
            show_shapes = True,\
            show_dtype = True,\
            show_layer_names = True,\
            expand_nested = True,\
            show_layer_activations = True)

##### Import some necessary functions for loading the images #####
from tensorflow.keras.preprocessing import image
from numpy.linalg import norm

##### Define the function there all the images pass ONE by ONE and extract the features using ResNet50 architecture #####
def Extract_Feature(image_path, model):
  from tensorflow.keras.preprocessing import image
  ##### Target size should be equal to the initial defined architecture of ResNet50 #####
  Image_ = image.load_img(image_path, target_size = (128, 128))
  ##### Convert Image to the NumPy array for training purpose #####
  Image_ = image.img_to_array(Image_)
  ##### Though, we are passing so, the dimension should be changed like (1, 128, 128, 3) #####
  Image_ = np.expand_dims(Image_, axis = 0)
  ##### processes_input is responsible to change the input in an appropriate way with regards to ResNet50 architecture needs #####
  Image_ = preprocess_input(Image_)
  ##### predict model will return the higher dimension, so flatten use to Convert to 1D array #####
  result = model.predict(Image_).flatten()
  
  ##### Prior to returning, should be normalized the distribution of the data points #####
  return (result/norm(result))

##### Check whether the function is working or not perfectly #####
from tensorflow.keras.preprocessing import image
image.load_img(IMAGE_PATH[100], target_size = (128, 128))

##### Extract features for all the images #####
from tensorflow.keras.preprocessing import image
from tqdm import tqdm
extracted_features = []
##### tqdm is used to process the performance parallel #####
for image in tqdm(IMAGE_PATH):
  ##### Appending the features into the list #####
  extracted_features.append(Extract_Feature(image, model))

##### Show the single input #####
cv2_imshow(cv2.imread('/content/images_128_128/018/0189691051.jpg'))

##### Use the same code that used in the Extract_Feature #####
from tensorflow.keras.preprocessing import image
##### Load the images #####
Image_ = image.load_img('/content/images_128_128/018/0189691051.jpg', target_size = (128, 128))
##### converting the images into NumPy array #####
Image_ = image.img_to_array(Image_)
##### Expand the dimension of that given input #####
Image_ = np.expand_dims(Image_, axis = 0)
##### Preprocess_input is used for making the input perfect for ResNet50 #####
Image_ = preprocess_input(Image_)
##### predict the model #####
result = model.predict(Image_).flatten()

##### Using, NearestNeighbours, we will find the closest point of that particular input #####
from sklearn.neighbors import NearestNeighbors
##### Initialize the algorithm with some use defined arguments #####
neighbours = NearestNeighbors(n_neighbors = 6, algorithm = 'brute')
##### fit the model for training purpose #####
neighbours.fit(extracted_features)

##### Find the distance and indices of the user input #####
distances, indices = neighbours.kneighbors([result])

print("The indices is {}".format(indices))

indices_ = np.array(indices).flatten()
print(indices_)

##### This will return top 5 recommendated pictures based on User input #####
figure = plt.figure(figsize = (15, 10))
number_row = 2
number_col = 3
plt.title('Fashion Recommendation items:\n\n'.title())
for index in range(1, (number_row*number_col + 1)):
  figure.add_subplot(number_row, number_col, index)
  plt.imshow(cv2.imread(IMAGE_PATH[indices_[index - 1]]))

plt.show()

##### Save the model and IMAGE_PATH for further use #####
import pickle
def save_model(parameter = None):
  with open('model.pkl', 'wb') as f:
    try:
      pickle.dump(model, f)
    except Exception as e:
      print(e.with_traceback)
    else:
      print('model saved successfully.'.capitalize())
      
##### Call the function from the current directory #####
if __name__ == "__main__":
  save_model()

##### Save the model and IMAGE_PATH for further use #####
import pickle
def save_image_file(parameter = None):
  with open('IMAGE_PATH.pkl', 'wb') as f:
    try:
      pickle.dump(IMAGE_PATH, f)
    except Exception as e:
      print(e.with_traceback)
    else:
      print('IMAGE_PATH saved successfully.'.capitalize())

##### Call the function from the current directory #####
if __name__ == "__main__":
  try:
    save_image_file()
  except Exception as e:
    print(e.with_traceback)

##### Save Extracted Feature for further use #####
import pickle
def extracted_features_save(parameter = None):
  with open('extracted_features.pkl', 'wb') as f:
    try:
      pickle.dump(extracted_features, f)
    except Exception as e:
      print(e.with_traceback)
    else:
      print('extracted features saved successfully.'.capitalize())

##### Call the function from the current directory #####
if __name__ == "__main__":
  try:
    extracted_features_save()
  except Exception as e:
    print(e.with_traceback)